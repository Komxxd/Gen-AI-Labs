# ğŸ§  Variational Autoencoder (VAE) â€“ Image Generation Lab

## ğŸ“Œ Objective

The goal of this lab is to **implement and train a Variational Autoencoder (VAE)** to:

* Learn a **latent representation** of image data
* Reconstruct input images accurately
* Generate **new, previously unseen images** by sampling from a learned probability distribution

In this experiment, we use the **MNIST handwritten digits dataset**.
By the end of training, the model is able to generate entirely new digit images that resemble real handwritten numbers.

---

## ğŸ¤– What is a Variational Autoencoder (VAE)?

A **Variational Autoencoder (VAE)** is a **generative deep learning model** that learns *how data is distributed*, rather than simply memorizing examples.

Unlike standard autoencoders, a VAE learns a **probability distribution** in its latent space, which allows us to:

* Sample new points
* Generate diverse outputs
* Interpolate smoothly between examples

---

### ğŸ”¹ Encoder

The **Encoder** compresses an input image into a *probabilistic latent representation*.

Instead of outputting a single latent vector, it outputs:

* **Mean (Î¼)** of the latent distribution
* **Log-variance (log ÏƒÂ²)** of the latent distribution

This tells the model *where* the image lies in latent space and *how uncertain* it is.

---

### ğŸ”¹ Reparameterization Trick

Directly sampling from a distribution breaks backpropagation.
To solve this, VAEs use the **reparameterization trick**:

<img width="1536" height="50" alt="image" src="https://github.com/user-attachments/assets/55b189a4-46da-48c8-bd01-fdeda7be1f57" />

This allows gradients to flow through randomness during training.

---

### ğŸ”¹ Decoder

The **Decoder** takes a latent vector `z` and tries to **reconstruct the original image**.

* Input: Latent vector (sampled from the learned distribution)
* Output: A 28Ã—28 grayscale image

The decoder is also what enables **image generation** by decoding randomly sampled latent vectors.

---

## ğŸ§© Whatâ€™s Happening in the Code?

### 1ï¸âƒ£ Dataset & Configuration

* **Dataset**: MNIST

  * 28Ã—28 grayscale handwritten digits
* **Batch Size**: 128
* **Epochs**: 10
* **Latent Dimension**: 20
* **Optimizer**: Adam
* **Learning Rate**: 0.001

Images are converted to tensors in the range **[0, 1]**, which matches the decoderâ€™s `Sigmoid` output.

---

### 2ï¸âƒ£ VAE Architecture

**Encoder**

* Input: 784 (flattened image)
* Hidden layer: 400 neurons (ReLU)
* Outputs:

  * Mean (Î¼)
  * Log-variance (log ÏƒÂ²)

**Decoder**

* Input: Latent vector (size = 20)
* Hidden layer: 400 neurons (ReLU)
* Output: 784 values â†’ reshaped to 28Ã—28

---

### 3ï¸âƒ£ Loss Function

The VAE uses a **combined loss function**:

#### ğŸ”¹ Reconstruction Loss (Binary Cross-Entropy)

* Measures how close the reconstructed image is to the original

#### ğŸ”¹ KL Divergence Loss

* Forces the latent space to resemble a **standard normal distribution**
* Enables smooth sampling and interpolation

**Total Loss = Reconstruction Loss + KL Divergence**

---

### 4ï¸âƒ£ Training Loop

For each epoch:

1. Encode the input image into `(Î¼, log ÏƒÂ²)`
2. Sample a latent vector using the reparameterization trick
3. Decode the latent vector to reconstruct the image
4. Compute total VAE loss
5. Update model weights using backpropagation

As training progresses:

* Reconstructions become clearer
* The latent space becomes well-structured

---

### ğŸ–¼ï¸ 1ï¸âƒ£ Reconstruction Results

Original MNIST images (top row) vs reconstructed images (bottom row):

<img width="640" height="253" alt="image" src="https://github.com/user-attachments/assets/99fe145d-2c5b-4d03-89fd-b391f1230c79" />

ğŸ“Œ **Observation**
The reconstructed digits closely resemble the originals, though they appear slightly blurred â€” a known and expected behavior of VAEs due to probabilistic decoding.

---

### ğŸ§ª 2ï¸âƒ£ Generated Samples

Random latent vectors sampled from a standard normal distribution and decoded into images:

<img width="329" height="328" alt="image" src="https://github.com/user-attachments/assets/5c0464f6-2840-4f38-ad4b-57d6d37cbf6f" />


ğŸ“Œ **Observation**
These digits were **never seen during training**.
They are entirely new samples generated by the model, demonstrating successful learning of the data distribution.

---

### ğŸ“ˆ 3ï¸âƒ£ Training Loss Curve

VAE loss plotted over training epochs:

<img width="571" height="455" alt="image" src="https://github.com/user-attachments/assets/43517703-2852-49cd-92fc-5bb4939a362a" />


ğŸ“Œ **Observation**
The steadily decreasing loss indicates stable training and improved reconstruction quality over time.

---
